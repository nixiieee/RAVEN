{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2f7db8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model(model, dataloader, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    accuracy_metric  = evaluate.load(\"accuracy\")\n",
    "    precision_metric = evaluate.load(\"precision\")\n",
    "    recall_metric    = evaluate.load(\"recall\")\n",
    "    f1_metric        = evaluate.load(\"f1\")\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_metric.compute(predictions=all_preds, references=all_labels)[\"accuracy\"]\n",
    "    bal_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_metric.compute(predictions=all_preds, references=all_labels, average=\"macro\")[\"precision\"]\n",
    "    rec = recall_metric.compute(predictions=all_preds, references=all_labels, average=\"macro\")[\"recall\"]\n",
    "    f1 = f1_metric.compute(predictions=all_preds, references=all_labels, average=\"macro\")[\"f1\"]\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"balanced_accuracy\": bal_acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad378162",
   "metadata": {},
   "source": [
    "# Model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb12ae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForEncoderClassification:\n",
    "    processor: Any \n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{'input_features': feature['input_features']} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors='pt')\n",
    "        \n",
    "        batch['labels'] = torch.tensor(\n",
    "            [feature['labels'] for feature in features],\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "779cabe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers import AutoConfig, PreTrainedModel, WhisperProcessor, WhisperModel\n",
    "import torch.nn as nn \n",
    "\n",
    "class WhisperClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels=5, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.pool_norm = nn.LayerNorm(hidden_size)\n",
    "        self.pre_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        mid1 = max(hidden_size // 2, num_labels * 4)\n",
    "        mid2 = max(hidden_size // 4, num_labels * 2)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, mid1),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LayerNorm(mid1),\n",
    "            nn.Linear(mid1, mid2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LayerNorm(mid2),\n",
    "            nn.Linear(mid2, num_labels),\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        if attention_mask is not None:\n",
    "            lengths = attention_mask.sum(dim=1, keepdim=True)\n",
    "            masked = hidden_states * attention_mask.unsqueeze(-1)\n",
    "            pooled = masked.sum(dim=1) / lengths\n",
    "        else:\n",
    "            pooled = hidden_states.mean(dim=1)\n",
    "        x = self.pool_norm(pooled)\n",
    "        x = self.pre_dropout(x)\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "\n",
    "class WhisperForEmotionClassification(PreTrainedModel):\n",
    "    config_class = AutoConfig\n",
    "\n",
    "    def __init__(\n",
    "        self, config, model_name, num_labels=5, dropout=0.2\n",
    "    ):\n",
    "        super().__init__(config)\n",
    "        self.encoder = WhisperModel.from_pretrained(model_name).encoder\n",
    "        hidden_size = config.hidden_size\n",
    "        self.classifier = WhisperClassifier(\n",
    "            hidden_size, num_labels=num_labels, dropout=dropout\n",
    "        )\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_features, attention_mask=None, labels=None):\n",
    "        encoder_output = self.encoder(\n",
    "            input_features=input_features,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        hidden_states = encoder_output.last_hidden_state\n",
    "        logits = self.classifier(hidden_states, attention_mask=attention_mask)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss()(\n",
    "                logits.view(-1, logits.size(-1)), labels.view(-1)\n",
    "            )\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c23ba73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Tuple, Dict, Optional\n",
    "import torch\n",
    "import gigaam\n",
    "\n",
    "class GigaEmotionInferencer:\n",
    "    \"\"\"\n",
    "    Wrapper for GigaAM emotion classification model.\n",
    "    Methods:\n",
    "      - infer: return full probability dict\n",
    "      - predict_emotion: return top emotion index and its probability\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"emo\",\n",
    "        device: Union[str, torch.device] = \"cpu\",\n",
    "        sample_rate: Optional[int] = None,\n",
    "    ):\n",
    "        self.device = torch.device(device if isinstance(device, str) else device)\n",
    "        try:\n",
    "            self.model = gigaam.load_model(model_name)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load GigaAM model '{model_name}': {e}\")\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "    def infer(\n",
    "        self,\n",
    "        input: str\n",
    "    ) -> Dict[str, float]:\n",
    "        try:\n",
    "            probs = self.model.get_probs(input)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Emotion inference failed: {e}\")\n",
    "\n",
    "        return {emotion: float(prob) for emotion, prob in probs.items()}\n",
    "\n",
    "    def predict_emotion(\n",
    "        self,\n",
    "        input: Union[str, torch.Tensor]\n",
    "    ) -> Tuple[int, float]:\n",
    "        probs = self.infer(input)\n",
    "        top_emotion = max(probs, key=probs.get)\n",
    "        return top_emotion, probs[top_emotion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "798c2595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    PreTrainedModel,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoProcessor,\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels=5, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.pool_norm = nn.LayerNorm(hidden_size)\n",
    "        self.pre_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        mid1 = max(hidden_size // 2, num_labels * 4)\n",
    "        mid2 = max(hidden_size // 4, num_labels * 2)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, mid1),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LayerNorm(mid1),\n",
    "            nn.Linear(mid1, mid2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LayerNorm(mid2),\n",
    "            nn.Linear(mid2, num_labels),\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        if attention_mask is not None:\n",
    "            lengths = attention_mask.sum(dim=1, keepdim=True)\n",
    "            masked = hidden_states * attention_mask.unsqueeze(-1)\n",
    "            pooled = masked.sum(dim=1) / lengths\n",
    "        else:\n",
    "            pooled = hidden_states.mean(dim=1)\n",
    "        x = self.pool_norm(pooled)\n",
    "        x = self.pre_dropout(x)\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "    \n",
    "class ModelForEmotionClassification(PreTrainedModel):\n",
    "    config_class = AutoConfig\n",
    "\n",
    "    def __init__(\n",
    "        self, config, model_name, num_labels=5, dropout=0.2\n",
    "    ):\n",
    "        super().__init__(config)\n",
    "        self.encoder = AutoModel.from_pretrained(model_name, trust_remote_code=True).model.encoder\n",
    "        hidden_size = config.encoder['d_model']\n",
    "        self.classifier = EmotionClassifier(\n",
    "            hidden_size, num_labels=num_labels, dropout=dropout\n",
    "        )\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_features: torch.Tensor,\n",
    "        input_lengths: torch.Tensor,\n",
    "        attention_mask: torch.Tensor = None,\n",
    "        labels: torch.Tensor = None\n",
    "    ) -> SequenceClassifierOutput:\n",
    "        encoded, out_lens = self.encoder(input_features, input_lengths)\n",
    "        hidden_states = encoded.transpose(1, 2)\n",
    "\n",
    "        if attention_mask is None:\n",
    "            max_t = hidden_states.size(1)\n",
    "            attention_mask = (\n",
    "                torch.arange(max_t, device=out_lens.device)\n",
    "                .unsqueeze(0)\n",
    "                .lt(out_lens.unsqueeze(1))\n",
    "                .long()\n",
    "            )\n",
    "\n",
    "        logits = self.classifier(hidden_states, attention_mask=attention_mask)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss, logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "42164d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "@dataclass\n",
    "class DataCollatorForEncoderClassificationGigaAM:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(\n",
    "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        tensors = [\n",
    "            f[\"input_features\"]\n",
    "            if isinstance(f[\"input_features\"], torch.Tensor)\n",
    "            else torch.tensor(f[\"input_features\"], dtype=torch.float32)\n",
    "            for f in features\n",
    "        ]\n",
    "        seq_lens = [t.shape[0] for t in tensors]\n",
    "        assert len(set(seq_lens)) == 1, \"Все sequences в батче должны быть одинаковой длины\"\n",
    "        batch_inputs = torch.stack(tensors, dim=0)  # shape: [B, T, feat_in]\n",
    "\n",
    "        batch_labels = torch.tensor(\n",
    "            [f[\"labels\"] for f in features], dtype=torch.long\n",
    "        )\n",
    "        batch_lens = torch.tensor(\n",
    "            [f[\"input_lengths\"] for f in features], dtype=torch.long\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_features\": batch_inputs,\n",
    "            \"input_lengths\": batch_lens,\n",
    "            \"labels\": batch_labels,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d50e7c",
   "metadata": {},
   "source": [
    "# dusha test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96f5028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dusha_test_dataset = load_dataset(\"nixiieee/dusha_balanced\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec618097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'emotion'],\n",
       "    num_rows: 3601\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dusha_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb939bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': 'c7ecb2e501e4bcf41b714cd93e720368.wav',\n",
       " 'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        1.83105469e-04, 6.10351562e-05, 1.22070312e-04], shape=(105600,)),\n",
       " 'sampling_rate': 16000}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dusha_test_dataset['audio'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222b3990",
   "metadata": {},
   "source": [
    "## Whisper-based classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cde9313",
   "metadata": {},
   "source": [
    "### whisper small simple classfifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dc48128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForAudioClassification, WhisperProcessor\n",
    "\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    \"/home/llm_agent/video_audio_pipeline/emotion_dusha/whisper-emotion-classfication-layernorm/checkpoint-14094\"\n",
    ")\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04d72043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3b591f6810439c9c51fa807b86230b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/3601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# label_mapping = {'neutral': 0, 'angry': 1, 'positive': 2, 'sad': 3, 'other': 4}\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch['audio']\n",
    "    batch['input_features'] = processor(audio['array'], sampling_rate=audio['sampling_rate']).input_features[0]\n",
    "    batch['labels'] = batch[\"emotion\"]\n",
    "    # batch['labels'] = label_mapping[batch[\"emotion\"]] #[label_mapping[emotion] for emotion in batch[\"emotion\"]]\n",
    "    return batch\n",
    "\n",
    "processed_dusha = dusha_test_dataset.map(\n",
    "    prepare_dataset,\n",
    "    num_proc=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de906b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "data_collator = DataCollatorForEncoderClassification(processor)\n",
    "dataloader = DataLoader(processed_dusha, batch_size=16, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3e8aa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 226/226 [09:31<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:           71.37%\n",
      "Balanced accuracy:  70.58%\n",
      "Precision:          82.59%\n",
      "Recall:             70.58%\n",
      "F1 Score:           74.14%\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_model(model, dataloader)\n",
    "print(f\"Accuracy:           {metrics['accuracy']:.2%}\")\n",
    "print(f\"Balanced accuracy:  {metrics['balanced_accuracy']:.2%}\")\n",
    "print(f\"Precision:          {metrics['precision']:.2%}\")\n",
    "print(f\"Recall:             {metrics['recall']:.2%}\")\n",
    "print(f\"F1 Score:           {metrics['f1']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88151427",
   "metadata": {},
   "source": [
    "### whisper small mlp classifier (dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bd4ca2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WhisperModel were not initialized from the model checkpoint at nixiieee/whisper-small-emotion-classifier-dusha and are newly initialized: ['decoder.embed_positions.weight', 'decoder.embed_tokens.weight', 'decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.fc1.bias', 'decoder.layers.10.fc1.weight', 'decoder.layers.10.fc2.bias', 'decoder.layers.10.fc2.weight', 'decoder.layers.10.final_layer_norm.bias', 'decoder.layers.10.final_layer_norm.weight', 'decoder.layers.10.self_attn.k_proj.weight', 'decoder.layers.10.self_attn.out_proj.bias', 'decoder.layers.10.self_attn.out_proj.weight', 'decoder.layers.10.self_attn.q_proj.bias', 'decoder.layers.10.self_attn.q_proj.weight', 'decoder.layers.10.self_attn.v_proj.bias', 'decoder.layers.10.self_attn.v_proj.weight', 'decoder.layers.10.self_attn_layer_norm.bias', 'decoder.layers.10.self_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.fc1.bias', 'decoder.layers.11.fc1.weight', 'decoder.layers.11.fc2.bias', 'decoder.layers.11.fc2.weight', 'decoder.layers.11.final_layer_norm.bias', 'decoder.layers.11.final_layer_norm.weight', 'decoder.layers.11.self_attn.k_proj.weight', 'decoder.layers.11.self_attn.out_proj.bias', 'decoder.layers.11.self_attn.out_proj.weight', 'decoder.layers.11.self_attn.q_proj.bias', 'decoder.layers.11.self_attn.q_proj.weight', 'decoder.layers.11.self_attn.v_proj.bias', 'decoder.layers.11.self_attn.v_proj.weight', 'decoder.layers.11.self_attn_layer_norm.bias', 'decoder.layers.11.self_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.fc1.bias', 'decoder.layers.6.fc1.weight', 'decoder.layers.6.fc2.bias', 'decoder.layers.6.fc2.weight', 'decoder.layers.6.final_layer_norm.bias', 'decoder.layers.6.final_layer_norm.weight', 'decoder.layers.6.self_attn.k_proj.weight', 'decoder.layers.6.self_attn.out_proj.bias', 'decoder.layers.6.self_attn.out_proj.weight', 'decoder.layers.6.self_attn.q_proj.bias', 'decoder.layers.6.self_attn.q_proj.weight', 'decoder.layers.6.self_attn.v_proj.bias', 'decoder.layers.6.self_attn.v_proj.weight', 'decoder.layers.6.self_attn_layer_norm.bias', 'decoder.layers.6.self_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.fc1.bias', 'decoder.layers.7.fc1.weight', 'decoder.layers.7.fc2.bias', 'decoder.layers.7.fc2.weight', 'decoder.layers.7.final_layer_norm.bias', 'decoder.layers.7.final_layer_norm.weight', 'decoder.layers.7.self_attn.k_proj.weight', 'decoder.layers.7.self_attn.out_proj.bias', 'decoder.layers.7.self_attn.out_proj.weight', 'decoder.layers.7.self_attn.q_proj.bias', 'decoder.layers.7.self_attn.q_proj.weight', 'decoder.layers.7.self_attn.v_proj.bias', 'decoder.layers.7.self_attn.v_proj.weight', 'decoder.layers.7.self_attn_layer_norm.bias', 'decoder.layers.7.self_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.fc1.bias', 'decoder.layers.8.fc1.weight', 'decoder.layers.8.fc2.bias', 'decoder.layers.8.fc2.weight', 'decoder.layers.8.final_layer_norm.bias', 'decoder.layers.8.final_layer_norm.weight', 'decoder.layers.8.self_attn.k_proj.weight', 'decoder.layers.8.self_attn.out_proj.bias', 'decoder.layers.8.self_attn.out_proj.weight', 'decoder.layers.8.self_attn.q_proj.bias', 'decoder.layers.8.self_attn.q_proj.weight', 'decoder.layers.8.self_attn.v_proj.bias', 'decoder.layers.8.self_attn.v_proj.weight', 'decoder.layers.8.self_attn_layer_norm.bias', 'decoder.layers.8.self_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.fc1.bias', 'decoder.layers.9.fc1.weight', 'decoder.layers.9.fc2.bias', 'decoder.layers.9.fc2.weight', 'decoder.layers.9.final_layer_norm.bias', 'decoder.layers.9.final_layer_norm.weight', 'decoder.layers.9.self_attn.k_proj.weight', 'decoder.layers.9.self_attn.out_proj.bias', 'decoder.layers.9.self_attn.out_proj.weight', 'decoder.layers.9.self_attn.q_proj.bias', 'decoder.layers.9.self_attn.q_proj.weight', 'decoder.layers.9.self_attn.v_proj.bias', 'decoder.layers.9.self_attn.v_proj.weight', 'decoder.layers.9.self_attn_layer_norm.bias', 'decoder.layers.9.self_attn_layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"nixiieee/whisper-small-emotion-classifier-dusha\"\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", return_attention_mask=True)\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "model = WhisperForEmotionClassification.from_pretrained(pretrained_model_name_or_path=model_name, model_name=model_name, num_labels=5, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23f98cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 226/226 [10:00<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:           74.70%\n",
      "Balanced accuracy:  77.39%\n",
      "Precision:          79.01%\n",
      "Recall:             77.39%\n",
      "F1 Score:           77.93%\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_model(model, dataloader)\n",
    "print(f\"Accuracy:           {metrics['accuracy']:.2%}\")\n",
    "print(f\"Balanced accuracy:  {metrics['balanced_accuracy']:.2%}\")\n",
    "print(f\"Precision:          {metrics['precision']:.2%}\")\n",
    "print(f\"Recall:             {metrics['recall']:.2%}\")\n",
    "print(f\"F1 Score:           {metrics['f1']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780a2731",
   "metadata": {},
   "source": [
    "### whisper small mlp classifier (dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51d61e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da72dbec829e4e06a9f66a0221e92c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa456ed0e514a8897dd1e7a81a96f0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/354M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 226/226 [10:05<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:           76.87%\n",
      "Balanced accuracy:  79.30%\n",
      "Precision:          81.04%\n",
      "Recall:             79.30%\n",
      "F1 Score:           79.99%\n"
     ]
    }
   ],
   "source": [
    "model_name = \"nixiieee/whisper-small-emotion-classifier-dusha2\"\n",
    "model = WhisperForEmotionClassification.from_pretrained(model_name, num_labels=5, dropout=0.1)\n",
    "metrics = evaluate_model(model, dataloader)\n",
    "print(f\"Accuracy:           {metrics['accuracy']:.2%}\")\n",
    "print(f\"Balanced accuracy:  {metrics['balanced_accuracy']:.2%}\")\n",
    "print(f\"Precision:          {metrics['precision']:.2%}\")\n",
    "print(f\"Recall:             {metrics['recall']:.2%}\")\n",
    "print(f\"F1 Score:           {metrics['f1']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e10330e",
   "metadata": {},
   "source": [
    "### whisper large v3 turbo mlp classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01a43388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c6c3075f5c4e0b8f0a598e680ef981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/3601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v3-turbo\")\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch['audio']\n",
    "    batch['input_features'] = processor(audio['array'], sampling_rate=audio['sampling_rate']).input_features[0]\n",
    "    batch['labels'] = batch[\"emotion\"]\n",
    "    return batch\n",
    "\n",
    "processed_dusha = dusha_test_dataset.map(\n",
    "    prepare_dataset,\n",
    "    num_proc=32,\n",
    "    remove_columns=[\"audio\", \"emotion\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54ff2d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "data_collator = DataCollatorForEncoderClassification(processor)\n",
    "dataloader = DataLoader(processed_dusha, batch_size=16, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbe456a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WhisperModel were not initialized from the model checkpoint at nixiieee/whisper-large-v3-emotion-classifier-dusha and are newly initialized: ['decoder.embed_positions.weight', 'decoder.embed_tokens.weight', 'decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.self_attn_layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating: 100%|██████████| 226/226 [22:01<00:00,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:           78.87%\n",
      "Balanced accuracy:  81.55%\n",
      "Precision:          80.71%\n",
      "Recall:             81.55%\n",
      "F1 Score:           81.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"nixiieee/whisper-large-v3-emotion-classifier-dusha\"\n",
    "# config = AutoConfig.from_pretrained(model_name)\n",
    "model = WhisperForEmotionClassification.from_pretrained(pretrained_model_name_or_path=model_name, model_name=model_name, num_labels=5, dropout=0.05)\n",
    "\n",
    "metrics = evaluate_model(model, dataloader)\n",
    "print(f\"Accuracy:           {metrics['accuracy']:.2%}\")\n",
    "print(f\"Balanced accuracy:  {metrics['balanced_accuracy']:.2%}\")\n",
    "print(f\"Precision:          {metrics['precision']:.2%}\")\n",
    "print(f\"Recall:             {metrics['recall']:.2%}\")\n",
    "print(f\"F1 Score:           {metrics['f1']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b963d8",
   "metadata": {},
   "source": [
    "## Gigaam-EMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f56840b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/llm_agent/video_audio_pipeline/GigaAM/gigaam/__init__.py:118: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "audio_emotion_model = GigaEmotionInferencer(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2eb018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3601/3601 [04:28<00:00, 13.39it/s]\n",
      "/home/llm_agent/video_audio_pipeline/gigaam-env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.81%\n",
      "Balanced accuracy: 71.85%\n",
      "Precision: 69.85%\n",
      "Recall: 71.85%\n",
      "F1 Score: 70.81%\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "import tempfile\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "EMOTION_LABELS = {'neutral' : 0, 'angry' : 1, 'positive' : 2, 'sad' : 3, 'other' : 4}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sample in tqdm(dusha_test_dataset):\n",
    "        audio = sample['audio']['array']\n",
    "        sr = sample['audio']['sampling_rate']\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=True) as tmp:\n",
    "            wav_tmp_path = tmp.name\n",
    "            sf.write(wav_tmp_path, audio, samplerate=sr)\n",
    "            emo, _ = audio_emotion_model.predict_emotion(wav_tmp_path)\n",
    "        labels = sample[\"emotion\"]\n",
    "        all_predictions.append(EMOTION_LABELS[emo])\n",
    "        all_labels.append(labels)\n",
    "\n",
    "accuracy = accuracy_metric.compute(predictions=all_predictions, references=all_labels)\n",
    "balanced_accuracy = balanced_accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_metric.compute(predictions=all_predictions, references=all_labels, average=\"macro\")\n",
    "recall = recall_metric.compute(predictions=all_predictions, references=all_labels, average=\"macro\")\n",
    "f1 = f1_metric.compute(predictions=all_predictions, references=all_labels, average=\"macro\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy['accuracy']:.2%}\")\n",
    "print(f\"Balanced accuracy: {balanced_accuracy:.2%}\")\n",
    "print(f\"Precision: {precision['precision']:.2%}\")\n",
    "print(f\"Recall: {recall['recall']:.2%}\")\n",
    "print(f\"F1 Score: {f1['f1']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9061b7ad",
   "metadata": {},
   "source": [
    "## Gigaam mlp classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bcf775",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nixiieee/gigaam-rnnt-emotion-classifier-dusha\"\n",
    "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = ModelForEmotionClassification.from_pretrained(model_name, config=config, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34154f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a70748268a4b6eb5211e51901fd818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    processed = processor(\n",
    "        audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]\n",
    "    )   \n",
    "    batch[\"input_features\"] = processed[\"input_features\"][0]\n",
    "    batch[\"input_lengths\"] = processed[\"input_lengths\"][0]\n",
    "    batch[\"labels\"] = batch[\"emotion\"]\n",
    "    return batch\n",
    "\n",
    "processed_dusha = dusha_test_dataset.map(prepare_dataset, remove_columns=['audio','emotion'], num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2208461e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForEncoderClassificationGigaAM(processor)\n",
    "dataloader = DataLoader(processed_dusha, batch_size=16, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "999cfb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 226/226 [08:36<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:           81.53%\n",
      "Balanced accuracy:  83.68%\n",
      "Precision:          84.44%\n",
      "Recall:             83.68%\n",
      "F1 Score:           84.03%\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_model(model, dataloader)\n",
    "print(f\"Accuracy:           {metrics['accuracy']:.2%}\")\n",
    "print(f\"Balanced accuracy:  {metrics['balanced_accuracy']:.2%}\")\n",
    "print(f\"Precision:          {metrics['precision']:.2%}\")\n",
    "print(f\"Recall:             {metrics['recall']:.2%}\")\n",
    "print(f\"F1 Score:           {metrics['f1']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2a5637",
   "metadata": {},
   "source": [
    "# resd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ec753dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "resd_dataset = load_dataset(\"Aniemore/resd\")\n",
    "resd_dataset = concatenate_datasets([resd_dataset[\"train\"], resd_dataset[\"test\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2019e7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': '32_happiness_enthusiasm_h_120',\n",
       " 'path': 'happiness_enthusiasm_32/32_happiness_enthusiasm_h_120.wav',\n",
       " 'emotion': 'happiness',\n",
       " 'speech': {'path': '32_happiness_enthusiasm_h_120.wav',\n",
       "  'array': array([-0.00018311, -0.00061035, -0.00076294, ...,  0.00085449,\n",
       "          0.00048828,  0.00030518], shape=(82211,)),\n",
       "  'sampling_rate': 16000}}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resd_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "eec56a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anger', 'disgust', 'enthusiasm', 'fear', 'happiness', 'neutral',\n",
       "       'sadness'], dtype='<U10')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(resd_dataset['emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b5ceab7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'fear': 223, 'anger': 219, 'happiness': 218, 'enthusiasm': 198, 'neutral': 191, 'disgust': 185, 'sadness': 162})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_label_dist(ds, label=\"emotion\"):\n",
    "    emotions_split = ds[label]\n",
    "    emotion_counts_split = Counter(emotions_split)\n",
    "    print(emotion_counts_split)\n",
    "    return\n",
    "\n",
    "get_label_dist(resd_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "365ce74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'happiness': 269, 'anger': 237, 'neutral': 231, 'sadness': 203, 'fear': 172, 'disgust': 161, 'enthusiasm': 123})\n"
     ]
    }
   ],
   "source": [
    "def remap_emotion(example):\n",
    "    emotion = example[\"emotion\"]\n",
    "    path = example[\"path\"]\n",
    "\n",
    "    if emotion in [\"fear\", \"disgust\", \"enthusiasm\"]:\n",
    "        # Извлекаем первую часть пути: 'happiness_enthusiasm_32'\n",
    "        base = path.split(\"/\")[0]\n",
    "        # Затем разбиваем её по `_` и берём эмоции\n",
    "        emotion_parts = base.split(\"_\")\n",
    "        \n",
    "        # Предположим, что первые два токена — это эмоции\n",
    "        if len(emotion_parts) >= 2:\n",
    "            new_emotion = emotion_parts[1]  # вторая эмоция\n",
    "            example[\"emotion\"] = new_emotion\n",
    "\n",
    "    return example\n",
    "\n",
    "resd_dataset = resd_dataset.map(remap_emotion)\n",
    "get_label_dist(resd_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f96ca4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541178acaf8c4f56a0b3f2b73e5db543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1396 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'happiness': 269, 'anger': 237, 'neutral': 231, 'sadness': 203, 'enthusiasm': 123})\n"
     ]
    }
   ],
   "source": [
    "def limit_class_samples(ds, label_field='label', class_limits=None, seed=42):\n",
    "    filtered_subsets = []\n",
    "\n",
    "    for class_value, max_count in class_limits.items():\n",
    "        subset = ds.filter(lambda x: x[label_field] == class_value)\n",
    "        subset = subset.select(range(min(len(subset), max_count)))\n",
    "        filtered_subsets.append(subset)\n",
    "\n",
    "    all_limited_classes = set(class_limits.keys())\n",
    "    remaining = ds.filter(lambda x: x[label_field] not in all_limited_classes)\n",
    "    filtered_subsets.append(remaining)\n",
    "\n",
    "    new_split = concatenate_datasets(filtered_subsets).shuffle(seed=seed)\n",
    "\n",
    "    return new_split\n",
    "\n",
    "resd_dataset = limit_class_samples(resd_dataset, label_field='emotion', class_limits={ \"disgust\" : 0, \"fear\" : 0})\n",
    "get_label_dist(resd_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fde37b5",
   "metadata": {},
   "source": [
    "## Whisper models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f82f7a",
   "metadata": {},
   "source": [
    "### whisper small simple classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3b68261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForAudioClassification, WhisperProcessor\n",
    "\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    \"/home/llm_agent/video_audio_pipeline/emotion_dusha/whisper-emotion-classfication-layernorm/checkpoint-14094\"\n",
    ")\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c797fa30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75b71740a3941479a6a3a2e1e8077fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1063 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_mapping = {'neutral': 0, 'anger': 1, 'happiness': 2, 'enthusiasm' : 2, 'sadness': 3}\n",
    "\n",
    "def preprocess_function(example):\n",
    "    audio = example['speech']\n",
    "    inputs = processor(audio[\"array\"], sampling_rate=16000)\n",
    "    example[\"input_features\"] = inputs.input_features[0]\n",
    "    example[\"labels\"] = label_mapping[example[\"emotion\"]]\n",
    "    return example\n",
    "\n",
    "processed_resd = resd_dataset.map(preprocess_function, num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5b23e043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2: 392, 1: 237, 0: 231, 3: 203})\n"
     ]
    }
   ],
   "source": [
    "get_label_dist(processed_resd, label='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8fc4a4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "data_collator = DataCollatorForEncoderClassification(processor)\n",
    "dataloader = DataLoader(processed_resd, batch_size=8, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d42c62ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 133/133 [03:41<00:00,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:           35.37%\n",
      "Balanced accuracy:  39.10%\n",
      "Precision:          33.34%\n",
      "Recall:             31.28%\n",
      "F1 Score:           29.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/llm_agent/video_audio_pipeline/emotion_dusha/gigaam_train/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2776: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "/home/llm_agent/video_audio_pipeline/emotion_dusha/gigaam_train/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_model(model, dataloader)\n",
    "print(f\"Accuracy:           {metrics['accuracy']:.2%}\")\n",
    "print(f\"Balanced accuracy:  {metrics['balanced_accuracy']:.2%}\")\n",
    "print(f\"Precision:          {metrics['precision']:.2%}\")\n",
    "print(f\"Recall:             {metrics['recall']:.2%}\")\n",
    "print(f\"F1 Score:           {metrics['f1']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd409a7",
   "metadata": {},
   "source": [
    "### whisper small mlp classifier (dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f03a882b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WhisperModel were not initialized from the model checkpoint at nixiieee/whisper-small-emotion-classifier-dusha and are newly initialized: ['decoder.embed_positions.weight', 'decoder.embed_tokens.weight', 'decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.fc1.bias', 'decoder.layers.10.fc1.weight', 'decoder.layers.10.fc2.bias', 'decoder.layers.10.fc2.weight', 'decoder.layers.10.final_layer_norm.bias', 'decoder.layers.10.final_layer_norm.weight', 'decoder.layers.10.self_attn.k_proj.weight', 'decoder.layers.10.self_attn.out_proj.bias', 'decoder.layers.10.self_attn.out_proj.weight', 'decoder.layers.10.self_attn.q_proj.bias', 'decoder.layers.10.self_attn.q_proj.weight', 'decoder.layers.10.self_attn.v_proj.bias', 'decoder.layers.10.self_attn.v_proj.weight', 'decoder.layers.10.self_attn_layer_norm.bias', 'decoder.layers.10.self_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.fc1.bias', 'decoder.layers.11.fc1.weight', 'decoder.layers.11.fc2.bias', 'decoder.layers.11.fc2.weight', 'decoder.layers.11.final_layer_norm.bias', 'decoder.layers.11.final_layer_norm.weight', 'decoder.layers.11.self_attn.k_proj.weight', 'decoder.layers.11.self_attn.out_proj.bias', 'decoder.layers.11.self_attn.out_proj.weight', 'decoder.layers.11.self_attn.q_proj.bias', 'decoder.layers.11.self_attn.q_proj.weight', 'decoder.layers.11.self_attn.v_proj.bias', 'decoder.layers.11.self_attn.v_proj.weight', 'decoder.layers.11.self_attn_layer_norm.bias', 'decoder.layers.11.self_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.fc1.bias', 'decoder.layers.6.fc1.weight', 'decoder.layers.6.fc2.bias', 'decoder.layers.6.fc2.weight', 'decoder.layers.6.final_layer_norm.bias', 'decoder.layers.6.final_layer_norm.weight', 'decoder.layers.6.self_attn.k_proj.weight', 'decoder.layers.6.self_attn.out_proj.bias', 'decoder.layers.6.self_attn.out_proj.weight', 'decoder.layers.6.self_attn.q_proj.bias', 'decoder.layers.6.self_attn.q_proj.weight', 'decoder.layers.6.self_attn.v_proj.bias', 'decoder.layers.6.self_attn.v_proj.weight', 'decoder.layers.6.self_attn_layer_norm.bias', 'decoder.layers.6.self_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.fc1.bias', 'decoder.layers.7.fc1.weight', 'decoder.layers.7.fc2.bias', 'decoder.layers.7.fc2.weight', 'decoder.layers.7.final_layer_norm.bias', 'decoder.layers.7.final_layer_norm.weight', 'decoder.layers.7.self_attn.k_proj.weight', 'decoder.layers.7.self_attn.out_proj.bias', 'decoder.layers.7.self_attn.out_proj.weight', 'decoder.layers.7.self_attn.q_proj.bias', 'decoder.layers.7.self_attn.q_proj.weight', 'decoder.layers.7.self_attn.v_proj.bias', 'decoder.layers.7.self_attn.v_proj.weight', 'decoder.layers.7.self_attn_layer_norm.bias', 'decoder.layers.7.self_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.fc1.bias', 'decoder.layers.8.fc1.weight', 'decoder.layers.8.fc2.bias', 'decoder.layers.8.fc2.weight', 'decoder.layers.8.final_layer_norm.bias', 'decoder.layers.8.final_layer_norm.weight', 'decoder.layers.8.self_attn.k_proj.weight', 'decoder.layers.8.self_attn.out_proj.bias', 'decoder.layers.8.self_attn.out_proj.weight', 'decoder.layers.8.self_attn.q_proj.bias', 'decoder.layers.8.self_attn.q_proj.weight', 'decoder.layers.8.self_attn.v_proj.bias', 'decoder.layers.8.self_attn.v_proj.weight', 'decoder.layers.8.self_attn_layer_norm.bias', 'decoder.layers.8.self_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.fc1.bias', 'decoder.layers.9.fc1.weight', 'decoder.layers.9.fc2.bias', 'decoder.layers.9.fc2.weight', 'decoder.layers.9.final_layer_norm.bias', 'decoder.layers.9.final_layer_norm.weight', 'decoder.layers.9.self_attn.k_proj.weight', 'decoder.layers.9.self_attn.out_proj.bias', 'decoder.layers.9.self_attn.out_proj.weight', 'decoder.layers.9.self_attn.q_proj.bias', 'decoder.layers.9.self_attn.q_proj.weight', 'decoder.layers.9.self_attn.v_proj.bias', 'decoder.layers.9.self_attn.v_proj.weight', 'decoder.layers.9.self_attn_layer_norm.bias', 'decoder.layers.9.self_attn_layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating: 100%|██████████| 133/133 [06:02<00:00,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:           34.81%\n",
      "Balanced accuracy:  37.83%\n",
      "Precision:          33.64%\n",
      "Recall:             30.27%\n",
      "F1 Score:           27.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/llm_agent/video_audio_pipeline/emotion_dusha/gigaam_train/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2776: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "/home/llm_agent/video_audio_pipeline/emotion_dusha/gigaam_train/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "model_name = \"nixiieee/whisper-small-emotion-classifier-dusha\"\n",
    "model = WhisperForEmotionClassification.from_pretrained(pretrained_model_name_or_path=model_name, model_name=model_name, num_labels=5, dropout=0.2)\n",
    "\n",
    "metrics = evaluate_model(model, dataloader)\n",
    "print(f\"Accuracy:           {metrics['accuracy']:.2%}\")\n",
    "print(f\"Balanced accuracy:  {metrics['balanced_accuracy']:.2%}\")\n",
    "print(f\"Precision:          {metrics['precision']:.2%}\")\n",
    "print(f\"Recall:             {metrics['recall']:.2%}\")\n",
    "print(f\"F1 Score:           {metrics['f1']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc087fe",
   "metadata": {},
   "source": [
    "### whisper small mlp classifier (dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "933900a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WhisperModel were not initialized from the model checkpoint at nixiieee/whisper-small-emotion-classifier-dusha2 and are newly initialized: ['decoder.embed_positions.weight', 'decoder.embed_tokens.weight', 'decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.fc1.bias', 'decoder.layers.10.fc1.weight', 'decoder.layers.10.fc2.bias', 'decoder.layers.10.fc2.weight', 'decoder.layers.10.final_layer_norm.bias', 'decoder.layers.10.final_layer_norm.weight', 'decoder.layers.10.self_attn.k_proj.weight', 'decoder.layers.10.self_attn.out_proj.bias', 'decoder.layers.10.self_attn.out_proj.weight', 'decoder.layers.10.self_attn.q_proj.bias', 'decoder.layers.10.self_attn.q_proj.weight', 'decoder.layers.10.self_attn.v_proj.bias', 'decoder.layers.10.self_attn.v_proj.weight', 'decoder.layers.10.self_attn_layer_norm.bias', 'decoder.layers.10.self_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.fc1.bias', 'decoder.layers.11.fc1.weight', 'decoder.layers.11.fc2.bias', 'decoder.layers.11.fc2.weight', 'decoder.layers.11.final_layer_norm.bias', 'decoder.layers.11.final_layer_norm.weight', 'decoder.layers.11.self_attn.k_proj.weight', 'decoder.layers.11.self_attn.out_proj.bias', 'decoder.layers.11.self_attn.out_proj.weight', 'decoder.layers.11.self_attn.q_proj.bias', 'decoder.layers.11.self_attn.q_proj.weight', 'decoder.layers.11.self_attn.v_proj.bias', 'decoder.layers.11.self_attn.v_proj.weight', 'decoder.layers.11.self_attn_layer_norm.bias', 'decoder.layers.11.self_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.fc1.bias', 'decoder.layers.6.fc1.weight', 'decoder.layers.6.fc2.bias', 'decoder.layers.6.fc2.weight', 'decoder.layers.6.final_layer_norm.bias', 'decoder.layers.6.final_layer_norm.weight', 'decoder.layers.6.self_attn.k_proj.weight', 'decoder.layers.6.self_attn.out_proj.bias', 'decoder.layers.6.self_attn.out_proj.weight', 'decoder.layers.6.self_attn.q_proj.bias', 'decoder.layers.6.self_attn.q_proj.weight', 'decoder.layers.6.self_attn.v_proj.bias', 'decoder.layers.6.self_attn.v_proj.weight', 'decoder.layers.6.self_attn_layer_norm.bias', 'decoder.layers.6.self_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.fc1.bias', 'decoder.layers.7.fc1.weight', 'decoder.layers.7.fc2.bias', 'decoder.layers.7.fc2.weight', 'decoder.layers.7.final_layer_norm.bias', 'decoder.layers.7.final_layer_norm.weight', 'decoder.layers.7.self_attn.k_proj.weight', 'decoder.layers.7.self_attn.out_proj.bias', 'decoder.layers.7.self_attn.out_proj.weight', 'decoder.layers.7.self_attn.q_proj.bias', 'decoder.layers.7.self_attn.q_proj.weight', 'decoder.layers.7.self_attn.v_proj.bias', 'decoder.layers.7.self_attn.v_proj.weight', 'decoder.layers.7.self_attn_layer_norm.bias', 'decoder.layers.7.self_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.fc1.bias', 'decoder.layers.8.fc1.weight', 'decoder.layers.8.fc2.bias', 'decoder.layers.8.fc2.weight', 'decoder.layers.8.final_layer_norm.bias', 'decoder.layers.8.final_layer_norm.weight', 'decoder.layers.8.self_attn.k_proj.weight', 'decoder.layers.8.self_attn.out_proj.bias', 'decoder.layers.8.self_attn.out_proj.weight', 'decoder.layers.8.self_attn.q_proj.bias', 'decoder.layers.8.self_attn.q_proj.weight', 'decoder.layers.8.self_attn.v_proj.bias', 'decoder.layers.8.self_attn.v_proj.weight', 'decoder.layers.8.self_attn_layer_norm.bias', 'decoder.layers.8.self_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.fc1.bias', 'decoder.layers.9.fc1.weight', 'decoder.layers.9.fc2.bias', 'decoder.layers.9.fc2.weight', 'decoder.layers.9.final_layer_norm.bias', 'decoder.layers.9.final_layer_norm.weight', 'decoder.layers.9.self_attn.k_proj.weight', 'decoder.layers.9.self_attn.out_proj.bias', 'decoder.layers.9.self_attn.out_proj.weight', 'decoder.layers.9.self_attn.q_proj.bias', 'decoder.layers.9.self_attn.q_proj.weight', 'decoder.layers.9.self_attn.v_proj.bias', 'decoder.layers.9.self_attn.v_proj.weight', 'decoder.layers.9.self_attn_layer_norm.bias', 'decoder.layers.9.self_attn_layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating: 100%|██████████| 133/133 [06:12<00:00,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:           36.22%\n",
      "Balanced accuracy:  39.22%\n",
      "Precision:          34.69%\n",
      "Recall:             31.38%\n",
      "F1 Score:           28.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/llm_agent/video_audio_pipeline/emotion_dusha/gigaam_train/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2776: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "/home/llm_agent/video_audio_pipeline/emotion_dusha/gigaam_train/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "model_name = \"nixiieee/whisper-small-emotion-classifier-dusha2\"\n",
    "model = WhisperForEmotionClassification.from_pretrained(pretrained_model_name_or_path=model_name, model_name=model_name, num_labels=5, dropout=0.1)\n",
    "\n",
    "metrics = evaluate_model(model, dataloader)\n",
    "print(f\"Accuracy:           {metrics['accuracy']:.2%}\")\n",
    "print(f\"Balanced accuracy:  {metrics['balanced_accuracy']:.2%}\")\n",
    "print(f\"Precision:          {metrics['precision']:.2%}\")\n",
    "print(f\"Recall:             {metrics['recall']:.2%}\")\n",
    "print(f\"F1 Score:           {metrics['f1']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a41361",
   "metadata": {},
   "source": [
    "### whisper large v3 turbo mlp classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5dbeb3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359da6d768ec49438da686b0a1aaeab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1063 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v3-turbo\")\n",
    "\n",
    "def prepare_dataset(example):\n",
    "    audio = example['speech']\n",
    "    inputs = processor(audio[\"array\"], sampling_rate=16000)\n",
    "    example[\"input_features\"] = inputs.input_features[0]\n",
    "    example[\"labels\"] = label_mapping[example[\"emotion\"]]\n",
    "    return example\n",
    "\n",
    "processed_resd = resd_dataset.map(\n",
    "    prepare_dataset,\n",
    "    num_proc=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f23ecd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "data_collator = DataCollatorForEncoderClassification(processor)\n",
    "dataloader = DataLoader(processed_resd, batch_size=16, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d804bf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WhisperModel were not initialized from the model checkpoint at nixiieee/whisper-large-v3-emotion-classifier-dusha and are newly initialized: ['decoder.embed_positions.weight', 'decoder.embed_tokens.weight', 'decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.self_attn_layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating: 100%|██████████| 67/67 [31:33<00:00, 28.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:           27.38%\n",
      "Balanced accuracy:  26.95%\n",
      "Precision:          35.55%\n",
      "Recall:             21.56%\n",
      "F1 Score:           26.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/llm_agent/video_audio_pipeline/emotion_dusha/gigaam_train/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2776: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "/home/llm_agent/video_audio_pipeline/emotion_dusha/gigaam_train/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "model_name = \"nixiieee/whisper-large-v3-emotion-classifier-dusha\"\n",
    "# config = AutoConfig.from_pretrained(model_name)\n",
    "model = WhisperForEmotionClassification.from_pretrained(pretrained_model_name_or_path=model_name, model_name=model_name, num_labels=5, dropout=0.05)\n",
    "\n",
    "metrics = evaluate_model(model, dataloader)\n",
    "print(f\"Accuracy:           {metrics['accuracy']:.2%}\")\n",
    "print(f\"Balanced accuracy:  {metrics['balanced_accuracy']:.2%}\")\n",
    "print(f\"Precision:          {metrics['precision']:.2%}\")\n",
    "print(f\"Recall:             {metrics['recall']:.2%}\")=\n",
    "print(f\"F1 Score:           {metrics['f1']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d03abc",
   "metadata": {},
   "source": [
    "## Gigaam-EMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5f341d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/llm_agent/video_audio_pipeline/emotion_dusha/gigaam_train/lib/python3.11/site-packages/gigaam/__init__.py:118: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "audio_emotion_model = GigaEmotionInferencer(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7a7e0a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1063/1063 [01:25<00:00, 12.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 53.34%\n",
      "Balanced accuracy: 53.68%\n",
      "Precision: 57.65%\n",
      "Recall: 53.68%\n",
      "F1 Score: 50.84%\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "import tempfile\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "EMOTION_LABELS = {'neutral' : 0, 'angry' : 1, 'positive' : 2, 'sad' : 3, 'other' : 4}\n",
    "label_mapping = {'neutral': 0, 'anger': 1, 'happiness': 2, 'enthusiasm' : 2, 'sadness': 3, 'disgust' : 4, 'fear' : 4}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sample in tqdm(resd_dataset):\n",
    "        audio = sample['speech']['array']\n",
    "        sr = sample['speech']['sampling_rate']\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=True) as tmp:\n",
    "            wav_tmp_path = tmp.name\n",
    "            sf.write(wav_tmp_path, audio, samplerate=sr)\n",
    "            emo, _ = audio_emotion_model.predict_emotion(wav_tmp_path)\n",
    "        labels = sample[\"emotion\"]\n",
    "        all_predictions.append(EMOTION_LABELS[emo])\n",
    "        all_labels.append(label_mapping[labels])\n",
    "\n",
    "accuracy = accuracy_metric.compute(predictions=all_predictions, references=all_labels)\n",
    "balanced_accuracy = balanced_accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_metric.compute(predictions=all_predictions, references=all_labels, average=\"macro\")\n",
    "recall = recall_metric.compute(predictions=all_predictions, references=all_labels, average=\"macro\")\n",
    "f1 = f1_metric.compute(predictions=all_predictions, references=all_labels, average=\"macro\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy['accuracy']:.2%}\")\n",
    "print(f\"Balanced accuracy: {balanced_accuracy:.2%}\")\n",
    "print(f\"Precision: {precision['precision']:.2%}\")\n",
    "print(f\"Recall: {recall['recall']:.2%}\")\n",
    "print(f\"F1 Score: {f1['f1']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e8db73",
   "metadata": {},
   "source": [
    "## Gigaam mlp classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3965ee8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GigaAMRNNTHF were not initialized from the model checkpoint at nixiieee/gigaam-rnnt-emotion-classifier-dusha and are newly initialized: ['model.encoder.layers.0.conv.batch_norm.bias', 'model.encoder.layers.0.conv.batch_norm.num_batches_tracked', 'model.encoder.layers.0.conv.batch_norm.running_mean', 'model.encoder.layers.0.conv.batch_norm.running_var', 'model.encoder.layers.0.conv.batch_norm.weight', 'model.encoder.layers.0.conv.depthwise_conv.bias', 'model.encoder.layers.0.conv.depthwise_conv.weight', 'model.encoder.layers.0.conv.pointwise_conv1.bias', 'model.encoder.layers.0.conv.pointwise_conv1.weight', 'model.encoder.layers.0.conv.pointwise_conv2.bias', 'model.encoder.layers.0.conv.pointwise_conv2.weight', 'model.encoder.layers.0.feed_forward1.linear1.bias', 'model.encoder.layers.0.feed_forward1.linear1.weight', 'model.encoder.layers.0.feed_forward1.linear2.bias', 'model.encoder.layers.0.feed_forward1.linear2.weight', 'model.encoder.layers.0.feed_forward2.linear1.bias', 'model.encoder.layers.0.feed_forward2.linear1.weight', 'model.encoder.layers.0.feed_forward2.linear2.bias', 'model.encoder.layers.0.feed_forward2.linear2.weight', 'model.encoder.layers.0.norm_conv.bias', 'model.encoder.layers.0.norm_conv.weight', 'model.encoder.layers.0.norm_feed_forward1.bias', 'model.encoder.layers.0.norm_feed_forward1.weight', 'model.encoder.layers.0.norm_feed_forward2.bias', 'model.encoder.layers.0.norm_feed_forward2.weight', 'model.encoder.layers.0.norm_out.bias', 'model.encoder.layers.0.norm_out.weight', 'model.encoder.layers.0.norm_self_att.bias', 'model.encoder.layers.0.norm_self_att.weight', 'model.encoder.layers.0.self_attn.linear_k.bias', 'model.encoder.layers.0.self_attn.linear_k.weight', 'model.encoder.layers.0.self_attn.linear_out.bias', 'model.encoder.layers.0.self_attn.linear_out.weight', 'model.encoder.layers.0.self_attn.linear_q.bias', 'model.encoder.layers.0.self_attn.linear_q.weight', 'model.encoder.layers.0.self_attn.linear_v.bias', 'model.encoder.layers.0.self_attn.linear_v.weight', 'model.encoder.layers.1.conv.batch_norm.bias', 'model.encoder.layers.1.conv.batch_norm.num_batches_tracked', 'model.encoder.layers.1.conv.batch_norm.running_mean', 'model.encoder.layers.1.conv.batch_norm.running_var', 'model.encoder.layers.1.conv.batch_norm.weight', 'model.encoder.layers.1.conv.depthwise_conv.bias', 'model.encoder.layers.1.conv.depthwise_conv.weight', 'model.encoder.layers.1.conv.pointwise_conv1.bias', 'model.encoder.layers.1.conv.pointwise_conv1.weight', 'model.encoder.layers.1.conv.pointwise_conv2.bias', 'model.encoder.layers.1.conv.pointwise_conv2.weight', 'model.encoder.layers.1.feed_forward1.linear1.bias', 'model.encoder.layers.1.feed_forward1.linear1.weight', 'model.encoder.layers.1.feed_forward1.linear2.bias', 'model.encoder.layers.1.feed_forward1.linear2.weight', 'model.encoder.layers.1.feed_forward2.linear1.bias', 'model.encoder.layers.1.feed_forward2.linear1.weight', 'model.encoder.layers.1.feed_forward2.linear2.bias', 'model.encoder.layers.1.feed_forward2.linear2.weight', 'model.encoder.layers.1.norm_conv.bias', 'model.encoder.layers.1.norm_conv.weight', 'model.encoder.layers.1.norm_feed_forward1.bias', 'model.encoder.layers.1.norm_feed_forward1.weight', 'model.encoder.layers.1.norm_feed_forward2.bias', 'model.encoder.layers.1.norm_feed_forward2.weight', 'model.encoder.layers.1.norm_out.bias', 'model.encoder.layers.1.norm_out.weight', 'model.encoder.layers.1.norm_self_att.bias', 'model.encoder.layers.1.norm_self_att.weight', 'model.encoder.layers.1.self_attn.linear_k.bias', 'model.encoder.layers.1.self_attn.linear_k.weight', 'model.encoder.layers.1.self_attn.linear_out.bias', 'model.encoder.layers.1.self_attn.linear_out.weight', 'model.encoder.layers.1.self_attn.linear_q.bias', 'model.encoder.layers.1.self_attn.linear_q.weight', 'model.encoder.layers.1.self_attn.linear_v.bias', 'model.encoder.layers.1.self_attn.linear_v.weight', 'model.encoder.layers.10.conv.batch_norm.bias', 'model.encoder.layers.10.conv.batch_norm.num_batches_tracked', 'model.encoder.layers.10.conv.batch_norm.running_mean', 'model.encoder.layers.10.conv.batch_norm.running_var', 'model.encoder.layers.10.conv.batch_norm.weight', 'model.encoder.layers.10.conv.depthwise_conv.bias', 'model.encoder.layers.10.conv.depthwise_conv.weight', 'model.encoder.layers.10.conv.pointwise_conv1.bias', 'model.encoder.layers.10.conv.pointwise_conv1.weight', 'model.encoder.layers.10.conv.pointwise_conv2.bias', 'model.encoder.layers.10.conv.pointwise_conv2.weight', 'model.encoder.layers.10.feed_forward1.linear1.bias', 'model.encoder.layers.10.feed_forward1.linear1.weight', 'model.encoder.layers.10.feed_forward1.linear2.bias', 'model.encoder.layers.10.feed_forward1.linear2.weight', 'model.encoder.layers.10.feed_forward2.linear1.bias', 'model.encoder.layers.10.feed_forward2.linear1.weight', 'model.encoder.layers.10.feed_forward2.linear2.bias', 'model.encoder.layers.10.feed_forward2.linear2.weight', 'model.encoder.layers.10.norm_conv.bias', 'model.encoder.layers.10.norm_conv.weight', 'model.encoder.layers.10.norm_feed_forward1.bias', 'model.encoder.layers.10.norm_feed_forward1.weight', 'model.encoder.layers.10.norm_feed_forward2.bias', 'model.encoder.layers.10.norm_feed_forward2.weight', 'model.encoder.layers.10.norm_out.bias', 'model.encoder.layers.10.norm_out.weight', 'model.encoder.layers.10.norm_self_att.bias', 'model.encoder.layers.10.norm_self_att.weight', 'model.encoder.layers.10.self_attn.linear_k.bias', 'model.encoder.layers.10.self_attn.linear_k.weight', 'model.encoder.layers.10.self_attn.linear_out.bias', 'model.encoder.layers.10.self_attn.linear_out.weight', 'model.encoder.layers.10.self_attn.linear_q.bias', 'model.encoder.layers.10.self_attn.linear_q.weight', 'model.encoder.layers.10.self_attn.linear_v.bias', 'model.encoder.layers.10.self_attn.linear_v.weight', 'model.encoder.layers.11.conv.batch_norm.bias', 'model.encoder.layers.11.conv.batch_norm.num_batches_tracked', 'model.encoder.layers.11.conv.batch_norm.running_mean', 'model.encoder.layers.11.conv.batch_norm.running_var', 'model.encoder.layers.11.conv.batch_norm.weight', 'model.encoder.layers.11.conv.depthwise_conv.bias', 'model.encoder.layers.11.conv.depthwise_conv.weight', 'model.encoder.layers.11.conv.pointwise_conv1.bias', 'model.encoder.layers.11.conv.pointwise_conv1.weight', 'model.encoder.layers.11.conv.pointwise_conv2.bias', 'model.encoder.layers.11.conv.pointwise_conv2.weight', 'model.encoder.layers.11.feed_forward1.linear1.bias', 'model.encoder.layers.11.feed_forward1.linear1.weight', 'model.encoder.layers.11.feed_forward1.linear2.bias', 'model.encoder.layers.11.feed_forward1.linear2.weight', 'model.encoder.layers.11.feed_forward2.linear1.bias', 'model.encoder.layers.11.feed_forward2.linear1.weight', 'model.encoder.layers.11.feed_forward2.linear2.bias', 'model.encoder.layers.11.feed_forward2.linear2.weight', 'model.encoder.layers.11.norm_conv.bias', 'model.encoder.layers.11.norm_conv.weight', 'model.encoder.layers.11.norm_feed_forward1.bias', 'model.encoder.layers.11.norm_feed_forward1.weight', 'model.encoder.layers.11.norm_feed_forward2.bias', 'model.encoder.layers.11.norm_feed_forward2.weight', 'model.encoder.layers.11.norm_out.bias', 'model.encoder.layers.11.norm_out.weight', 'model.encoder.layers.11.norm_self_att.bias', 'model.encoder.layers.11.norm_self_att.weight', 'model.encoder.layers.11.self_attn.linear_k.bias', 'model.encoder.layers.11.self_attn.linear_k.weight', 'model.encoder.layers.11.self_attn.linear_out.bias', 'model.encoder.layers.11.self_attn.linear_out.weight', 'model.encoder.layers.11.self_attn.linear_q.bias', 'model.encoder.layers.11.self_attn.linear_q.weight', 'model.encoder.layers.11.self_attn.linear_v.bias', 'model.encoder.layers.11.self_attn.linear_v.weight', 'model.encoder.layers.12.conv.batch_norm.bias', 'model.encoder.layers.12.conv.batch_norm.num_batches_tracked', 'model.encoder.layers.12.conv.batch_norm.running_mean', 'model.encoder.layers.12.conv.batch_norm.running_var', 'model.encoder.layers.12.conv.batch_norm.weight', 'model.encoder.layers.12.conv.depthwise_conv.bias', 'model.encoder.layers.12.conv.depthwise_conv.weight', 'model.encoder.layers.12.conv.pointwise_conv1.bias', 'model.encoder.layers.12.conv.pointwise_conv1.weight', 'model.encoder.layers.12.conv.pointwise_conv2.bias', 'model.encoder.layers.12.conv.pointwise_conv2.weight', 'model.encoder.layers.12.feed_forward1.linear1.bias', 'model.encoder.layers.12.feed_forward1.linear1.weight', 'model.encoder.layers.12.feed_forward1.linear2.bias', 'model.encoder.layers.12.feed_forward1.linear2.weight', 'model.encoder.layers.12.feed_forward2.linear1.bias', 'model.encoder.layers.12.feed_forward2.linear1.weight', 'model.encoder.layers.12.feed_forward2.linear2.bias', 'model.encoder.layers.12.feed_forward2.linear2.weight', 'model.encoder.layers.12.norm_conv.bias', 'model.encoder.layers.12.norm_conv.weight', 'model.encoder.layers.12.norm_feed_forward1.bias', 'model.encoder.layers.12.norm_feed_forward1.weight', 'model.encoder.layers.12.norm_feed_forward2.bias', 'model.encoder.layers.12.norm_feed_forward2.weight', 'model.encoder.layers.12.norm_out.bias', 'model.encoder.layers.12.norm_out.weight', 'model.encoder.layers.12.norm_self_att.bias', 'model.encoder.layers.12.norm_self_att.weight', 'model.encoder.layers.12.self_attn.linear_k.bias', 'model.encoder.layers.12.self_attn.linear_k.weight', 'model.encoder.layers.12.self_attn.linear_out.bias', 'model.encoder.layers.12.self_attn.linear_out.weight', 'model.encoder.layers.12.self_attn.linear_q.bias', 'model.encoder.layers.12.self_attn.linear_q.weight', 'model.encoder.layers.12.self_attn.linear_v.bias', 'model.encoder.layers.12.self_attn.linear_v.weight', 'model.encoder.layers.13.conv.batch_norm.bias', 'model.encoder.layers.13.conv.batch_norm.num_batches_tracked', 'model.encoder.layers.13.conv.batch_norm.running_mean', 'model.encoder.layers.13.conv.batch_norm.running_var', 'model.encoder.layers.13.conv.batch_norm.weight', 'model.encoder.layers.13.conv.depthwise_conv.bias', 'model.encoder.layers.13.conv.depthwise_conv.weight', 'model.encoder.layers.13.conv.pointwise_conv1.bias', 'model.encoder.layers.13.conv.pointwise_conv1.weight', 'model.encoder.layers.13.conv.pointwise_conv2.bias', 'model.encoder.layers.13.conv.pointwise_conv2.weight', 'model.encoder.layers.13.feed_forward1.linear1.bias', 'model.encoder.layers.13.feed_forward1.linear1.weight', 'model.encoder.layers.13.feed_forward1.linear2.bias', 'model.encoder.layers.13.feed_forward1.linear2.weight', 'model.encoder.layers.13.feed_forward2.linear1.bias', 'model.encoder.layers.13.feed_forward2.linear1.weight', 'model.encoder.layers.13.feed_forward2.linear2.bias', 'model.encoder.layers.13.feed_forward2.linear2.weight', 'model.encoder.layers.13.norm_conv.bias', 'model.encoder.layers.13.norm_conv.weight', 'model.encoder.layers.13.norm_feed_forward1.bias', 'model.encoder.layers.13.norm_feed_forward1.weight', 'model.encoder.layers.13.norm_feed_forward2.bias', 'model.encoder.layers.13.norm_feed_forward2.weight', 'model.encoder.layers.13.norm_out.bias', 'model.encoder.layers.13.norm_out.weight', 'model.encoder.layers.13.norm_self_att.bias', 'model.encoder.layers.13.norm_self_att.weight', 'model.encoder.layers.13.self_attn.linear_k.bias', 'model.encoder.layers.13.self_attn.linear_k.weight', 'model.encoder.layers.13.self_attn.linear_out.bias', 'model.encoder.layers.13.self_attn.linear_out.weight', 'model.encoder.layers.13.self_attn.linear_q.bias', 'model.encoder.layers.13.self_attn.linear_q.weight', 'model.encoder.layers.13.self_attn.linear_v.bias', 'model.encoder.layers.13.self_attn.linear_v.weight', 'model.encoder.layers.14.conv.batch_norm.bias', 'model.encoder.layers.14.conv.batch_norm.num_batches_tracked', 'model.encoder.layers.14.conv.batch_norm.running_mean', 'model.encoder.layers.14.conv.batch_norm.running_var', 'model.encoder.layers.14.conv.batch_norm.weight', 'model.encoder.layers.14.conv.depthwise_conv.bias', 'model.encoder.layers.14.conv.depthwise_conv.weight', 'model.encoder.layers.14.conv.pointwise_conv1.bias', 'model.encoder.layers.14.conv.pointwise_conv1.weight', 'model.encoder.layers.14.conv.pointwise_conv2.bias', 'model.encoder.layers.14.conv.pointwise_conv2.weight', 'model.encoder.layers.14.feed_forward1.linear1.bias', 'model.encoder.layers.14.feed_forward1.linear1.weight', 'model.encoder.layers.14.feed_forward1.linear2.bias', 'model.encoder.layers.14.feed_forward1.linear2.weight', 'model.encoder.layers.14.feed_forward2.linear1.bias', 'model.encoder.layers.14.feed_forward2.linear1.weight', 'model.encoder.layers.14.feed_forward2.linear2.bias', 'model.encoder.layers.14.feed_forward2.linear2.weight', 'model.encoder.layers.14.norm_conv.bias', 'model.encoder.layers.14.norm_conv.weight', 'model.encoder.layers.14.norm_feed_forward1.bias', 'model.encoder.layers.14.norm_feed_forward1.weight', 'model.encoder.layers.14.norm_feed_forward2.bias', 'model.encoder.layers.14.norm_feed_forward2.weight', 'model.encoder.layers.14.norm_out.bias', 'model.encoder.layers.14.norm_out.weight', 'model.encoder.layers.14.norm_self_att.bias', 'model.encoder.layers.14.norm_self_att.weight', 'model.encoder.layers.14.self_attn.linear_k.bias', 'model.encoder.layers.14.self_attn.linear_k.weight', 'model.encoder.layers.14.self_attn.linear_out.bias', 'model.encoder.layers.14.self_attn.linear_out.weight', 'model.encoder.layers.14.self_attn.linear_q.bias', 'model.encoder.layers.14.self_attn.linear_q.weight', 'model.encoder.layers.14.self_attn.linear_v.bias', 'model.encoder.layers.14.self_attn.linear_v.weight', 'model.encoder.layers.15.conv.batch_norm.bias', 'model.encoder.layers.15.conv.batch_norm.num_batches_tracked', 'model.encoder.layers.15.conv.batch_norm.running_mean', 'model.encoder.layers.15.conv.batch_norm.running_var', 'model.encoder.layers.15.conv.batch_norm.weight', 'model.encoder.layers.15.conv.depthwise_conv.bias', 'model.encoder.layers.15.conv.depthwise_conv.weight', 'model.encoder.layers.15.conv.pointwise_conv1.bias', 'model.encoder.layers.15.conv.pointwise_conv1.weight', 'model.encoder.layers.15.conv.pointwise_conv2.bias', 'model.encoder.layers.15.conv.pointwise_conv2.weight', 'model.encoder.layers.15.feed_forward1.linear1.bias', 'model.encoder.layers.15.feed_forward1.linear1.weight', 'model.encoder.layers.15.feed_forward1.linear2.bias', 'model.encoder.layers.15.feed_forward1.linear2.weight', 'model.encoder.layers.15.feed_forward2.linear1.bias', 'model.encoder.layers.15.feed_forward2.linear1.weight', 'model.encoder.layers.15.feed_forward2.linear2.bias', 'model.encoder.layers.15.feed_forward2.linear2.weight', 'model.encoder.layers.15.norm_conv.bias', 'model.encoder.layers.15.norm_conv.weight', 'model.encoder.layers.15.norm_feed_forward1.bias', 'model.encoder.layers.15.norm_feed_forward1.weight', 'model.encoder.layers.15.norm_feed_forward2.bias', 'model.encoder.layers.15.norm_feed_forward2.weight', 'model.encoder.layers.15.norm_out.bias', 'model.encoder.layers.15.norm_out.weight', 'model.encoder.layers.15.norm_self_att.bias', 'model.encoder.layers.15.norm_self_att.weight', 'model.encoder.layers.15.self_attn.linear_k.bias', 'model.encoder.layers.15.self_attn.linear_k.weight', 'model.encoder.layers.15.self_attn.linear_out.bias', 'model.encoder.layers.15.self_attn.linear_out.weight', 'model.encoder.layers.15.self_attn.linear_q.bias', 'model.encoder.layers.15.self_attn.linear_q.weight', 'model.encoder.layers.15.self_attn.linear_v.bias', 'model.encoder.layers.15.self_attn.linear_v.weight', 'model.encoder.layers.2.conv.batch_norm.bias', 'model.encoder.layers.2.conv.batch_norm.num_batches_tracked', 'model.encoder.layers.2.conv.batch_norm.running_mean', 'model.encoder.layers.2.conv.batch_norm.running_var', 'model.encoder.layers.2.conv.batch_norm.weight', 'model.encoder.layers.2.conv.depthwise_conv.bias', 'model.encoder.layers.2.conv.depthwise_conv.weight', 'model.encoder.layers.2.conv.pointwise_conv1.bias', 'model.encoder.layers.2.conv.pointwise_conv1.weight', 'model.encoder.layers.2.conv.pointwise_conv2.bias', 'model.encoder.layers.2.conv.pointwise_conv2.weight', 'model.encoder.layers.2.feed_forward1.linear1.bias', 'model.encoder.layers.2.feed_forward1.linear1.weight', 'model.encoder.layers.2.feed_forward1.linear2.bias', 'model.encoder.layers.2.feed_forward1.linear2.weight', 'model.encoder.layers.2.feed_forward2.linear1.bias', 'model.encoder.layers.2.feed_forward2.linear1.weight', 'model.encoder.layers.2.feed_forward2.linear2.bias', 'model.encoder.layers.2.feed_forward2.linear2.weight', 'model.encoder.layers.2.norm_conv.bias', 'model.encoder.layers.2.norm_conv.weight', 'model.encoder.layers.2.norm_feed_forward1.bias', 'model.encoder.layers.2.norm_feed_forward1.weight', 'model.encoder.layers.2.norm_feed_forward2.bias', 'model.encoder.layers.2.norm_feed_forward2.weight', 'model.encoder.layers.2.norm_out.bias', 'model.encoder.layers.2.norm_out.weight', 'model.encoder.layers.2.norm_self_att.bias', 'model.encoder.layers.2.norm_self_att.weight', 'model.encoder.layers.2.self_attn.linear_k.bias', 'model.encoder.layers.2.self_attn.linear_k.weight', 'model.encoder.layers.2.self_attn.linear_out.bias', 'model.encoder.layers.2.self_attn.linear_out.weight', 'model.encoder.layers.2.self_attn.linear_q.bias', 'model.encoder.layers.2.self_attn.linear_q.weight', 'model.encoder.layers.2.self_attn.linear_v.bias', 'model.encoder.layers.2.self_attn.linear_v.weight', 'model.encoder.layers.3.conv.batch_norm.bias', 'model.encoder.layers.3.conv.batch_norm.num_batches_tracked', 'model.encoder.layers.3.conv.batch_norm.running_mean', 'model.encoder.layers.3.conv.batch_norm.running_var', 'model.encoder.layers.3.conv.batch_norm.weight', 'model.encoder.layers.3.conv.depthwise_conv.bias', 'model.encoder.layers.3.conv.depthwise_conv.weight', 'model.encoder.layers.3.conv.pointwise_conv1.bias', 'model.encoder.layers.3.conv.pointwise_conv1.weight', 'model.encoder.layers.3.conv.pointwise_conv2.bias', 'model.encoder.layers.3.conv.pointwise_conv2.weight', 'model.encoder.layers.3.feed_forward1.linear1.bias', 'model.encoder.layers.3.feed_forward1.linear1.weight', 'model.encoder.layers.3.feed_forward1.linear2.bias', 'model.encoder.layers.3.feed_forward1.linear2.weight', 'model.encoder.layers.3.feed_forward2.linear1.bias', 'model.encoder.layers.3.feed_forward2.linear1.weight', 'model.encoder.layers.3.feed_forward2.linear2.bias', 'model.encoder.layers.3.feed_forward2.linear2.weight', 'model.encoder.layers.3.norm_conv.bias', 'model.encoder.layers.3.norm_conv.weight', 'model.encoder.layers.3.norm_feed_forward1.bias', 'model.encoder.layers.3.norm_feed_forward1.weight', 'model.encoder.layers.3.norm_feed_forward2.bias', 'model.encoder.layers.3.norm_feed_forward2.weight', 'model.encoder.layers.3.norm_out.bias', 'model.encoder.layers.3.norm_out.weight', 'model.encoder.layers.3.norm_self_att.bias', 'model.encoder.layers.3.norm_self_att.weight', 'model.encoder.layers.3.self_attn.linear_k.bias', 'model.encoder.layers.3.self_attn.linear_k.weight', 'model.encoder.layers.3.self_attn.linear_out.bias', 'model.encoder.layers.3.self_attn.linear_out.weight', 'model.encoder.layers.3.self_attn.linear_q.bias', 'model.encoder.layers.3.self_attn.linear_q.weight', 'model.encoder.layers.3.self_attn.linear_v.bias', 'model.encoder.layers.3.self_attn.linear_v.weight', 'model.encoder.layers.4.conv.batch_norm.bias', 'model.encoder.layers.4.conv.batch_norm.num_batches_tracked', 'model.encoder.layers.4.conv.batch_norm.running_mean', 'model.encoder.layers.4.conv.batch_norm.running_var', 'model.encoder.layers.4.conv.batch_norm.weight', 'model.encoder.layers.4.conv.depthwise_conv.bias', 'model.encoder.layers.4.conv.depthwise_conv.weight', 'model.encoder.layers.4.conv.pointwise_conv1.bias', 'model.encoder.layers.4.conv.pointwise_conv1.weight', 'model.encoder.layers.4.conv.pointwise_conv2.bias', 'model.encoder.layers.4.conv.pointwise_conv2.weight', 'model.encoder.layers.4.feed_forward1.linear1.bias', 'model.encoder.layers.4.feed_forward1.linear1.weight', 'model.encoder.layers.4.feed_forward1.linear2.bias', 'model.encoder.layers.4.feed_forward1.linear2.weight', 'model.encoder.layers.4.feed_forward2.linear1.bias', 'model.encoder.layers.4.feed_forward2.linear1.weight', 'model.encoder.layers.4.feed_forward2.linear2.bias', 'model.encoder.layers.4.feed_forward2.linear2.weight', 'model.encoder.layers.4.norm_conv.bias', 'model.encoder.layers.4.norm_conv.weight', 'model.encoder.layers.4.norm_feed_forward1.bias', 'model.encoder.layers.4.norm_feed_forward1.weight', 'model.encoder.layers.4.norm_feed_forward2.bias', 'model.encoder.layers.4.norm_feed_forward2.weight', 'model.encoder.layers.4.norm_out.bias', 'model.encoder.layers.4.norm_out.weight', 'model.encoder.layers.4.norm_self_att.bias', 'model.encoder.layers.4.norm_self_att.weight', 'model.encoder.layers.4.self_attn.linear_k.bias', 'model.encoder.layers.4.self_attn.linear_k.weight', 'model.encoder.layers.4.self_attn.linear_out.bias', 'model.encoder.layers.4.self_attn.linear_out.weight', 'model.encoder.layers.4.self_attn.linear_q.bias', 'model.encoder.layers.4.self_attn.linear_q.weight', 'model.encoder.layers.4.self_attn.linear_v.bias', 'model.encoder.layers.4.self_attn.linear_v.weight', 'model.encoder.layers.5.conv.batch_norm.bias', 'model.encoder.layers.5.conv.batch_norm.num_batches_tracked', 'model.encoder.layers.5.conv.batch_norm.running_mean', 'model.encoder.layers.5.conv.batch_norm.running_var', 'model.encoder.layers.5.conv.batch_norm.weight', 'model.encoder.layers.5.conv.depthwise_conv.bias', 'model.encoder.layers.5.conv.depthwise_conv.weight', 'model.encoder.layers.5.conv.pointwise_conv1.bias', 'model.encoder.layers.5.conv.pointwise_conv1.weight', 'model.encoder.layers.5.conv.pointwise_conv2.bias', 'model.encoder.layers.5.conv.pointwise_conv2.weight', 'model.encoder.layers.5.feed_forward1.linear1.bias', 'model.encoder.layers.5.feed_forward1.linear1.weight', 'model.encoder.layers.5.feed_forward1.linear2.bias', 'model.encoder.layers.5.feed_forward1.linear2.weight', 'model.encoder.layers.5.feed_forward2.linear1.bias', 'model.encoder.layers.5.feed_forward2.linear1.weight', 'model.encoder.layers.5.feed_forward2.linear2.bias', 'model.encoder.layers.5.feed_forward2.linear2.weight', 'model.encoder.layers.5.norm_conv.bias', 'model.encoder.layers.5.norm_conv.weight', 'model.encoder.layers.5.norm_feed_forward1.bias', 'model.encoder.layers.5.norm_feed_forward1.weight', 'model.encoder.layers.5.norm_feed_forward2.bias', 'model.encoder.layers.5.norm_feed_forward2.weight', 'model.encoder.layers.5.norm_out.bias', 'model.encoder.layers.5.norm_out.weight', 'model.encoder.layers.5.norm_self_att.bias', 'model.encoder.layers.5.norm_self_att.weight', 'model.encoder.layers.5.self_attn.linear_k.bias', 'model.encoder.layers.5.self_attn.linear_k.weight', 'model.encoder.layers.5.self_attn.linear_out.bias', 'model.encoder.layers.5.self_attn.linear_out.weight', 'model.encoder.layers.5.self_attn.linear_q.bias', 'model.encoder.layers.5.self_attn.linear_q.weight', 'model.encoder.layers.5.self_attn.linear_v.bias', 'model.encoder.layers.5.self_attn.linear_v.weight', 'model.encoder.layers.6.conv.batch_norm.bias', 'model.encoder.layers.6.conv.batch_norm.num_batches_tracked', 'model.encoder.layers.6.conv.batch_norm.running_mean', 'model.encoder.layers.6.conv.batch_norm.running_var', 'model.encoder.layers.6.conv.batch_norm.weight', 'model.encoder.layers.6.conv.depthwise_conv.bias', 'model.encoder.layers.6.conv.depthwise_conv.weight', 'model.encoder.layers.6.conv.pointwise_conv1.bias', 'model.encoder.layers.6.conv.pointwise_conv1.weight', 'model.encoder.layers.6.conv.pointwise_conv2.bias', 'model.encoder.layers.6.conv.pointwise_conv2.weight', 'model.encoder.layers.6.feed_forward1.linear1.bias', 'model.encoder.layers.6.feed_forward1.linear1.weight', 'model.encoder.layers.6.feed_forward1.linear2.bias', 'model.encoder.layers.6.feed_forward1.linear2.weight', 'model.encoder.layers.6.feed_forward2.linear1.bias', 'model.encoder.layers.6.feed_forward2.linear1.weight', 'model.encoder.layers.6.feed_forward2.linear2.bias', 'model.encoder.layers.6.feed_forward2.linear2.weight', 'model.encoder.layers.6.norm_conv.bias', 'model.encoder.layers.6.norm_conv.weight', 'model.encoder.layers.6.norm_feed_forward1.bias', 'model.encoder.layers.6.norm_feed_forward1.weight', 'model.encoder.layers.6.norm_feed_forward2.bias', 'model.encoder.layers.6.norm_feed_forward2.weight', 'model.encoder.layers.6.norm_out.bias', 'model.encoder.layers.6.norm_out.weight', 'model.encoder.layers.6.norm_self_att.bias', 'model.encoder.layers.6.norm_self_att.weight', 'model.encoder.layers.6.self_attn.linear_k.bias', 'model.encoder.layers.6.self_attn.linear_k.weight', 'model.encoder.layers.6.self_attn.linear_out.bias', 'model.encoder.layers.6.self_attn.linear_out.weight', 'model.encoder.layers.6.self_attn.linear_q.bias', 'model.encoder.layers.6.self_attn.linear_q.weight', 'model.encoder.layers.6.self_attn.linear_v.bias', 'model.encoder.layers.6.self_attn.linear_v.weight', 'model.encoder.layers.7.conv.batch_norm.bias', 'model.encoder.layers.7.conv.batch_norm.num_batches_tracked', 'model.encoder.layers.7.conv.batch_norm.running_mean', 'model.encoder.layers.7.conv.batch_norm.running_var', 'model.encoder.layers.7.conv.batch_norm.weight', 'model.encoder.layers.7.conv.depthwise_conv.bias', 'model.encoder.layers.7.conv.depthwise_conv.weight', 'model.encoder.layers.7.conv.pointwise_conv1.bias', 'model.encoder.layers.7.conv.pointwise_conv1.weight', 'model.encoder.layers.7.conv.pointwise_conv2.bias', 'model.encoder.layers.7.conv.pointwise_conv2.weight', 'model.encoder.layers.7.feed_forward1.linear1.bias', 'model.encoder.layers.7.feed_forward1.linear1.weight', 'model.encoder.layers.7.feed_forward1.linear2.bias', 'model.encoder.layers.7.feed_forward1.linear2.weight', 'model.encoder.layers.7.feed_forward2.linear1.bias', 'model.encoder.layers.7.feed_forward2.linear1.weight', 'model.encoder.layers.7.feed_forward2.linear2.bias', 'model.encoder.layers.7.feed_forward2.linear2.weight', 'model.encoder.layers.7.norm_conv.bias', 'model.encoder.layers.7.norm_conv.weight', 'model.encoder.layers.7.norm_feed_forward1.bias', 'model.encoder.layers.7.norm_feed_forward1.weight', 'model.encoder.layers.7.norm_feed_forward2.bias', 'model.encoder.layers.7.norm_feed_forward2.weight', 'model.encoder.layers.7.norm_out.bias', 'model.encoder.layers.7.norm_out.weight', 'model.encoder.layers.7.norm_self_att.bias', 'model.encoder.layers.7.norm_self_att.weight', 'model.encoder.layers.7.self_attn.linear_k.bias', 'model.encoder.layers.7.self_attn.linear_k.weight', 'model.encoder.layers.7.self_attn.linear_out.bias', 'model.encoder.layers.7.self_attn.linear_out.weight', 'model.encoder.layers.7.self_attn.linear_q.bias', 'model.encoder.layers.7.self_attn.linear_q.weight', 'model.encoder.layers.7.self_attn.linear_v.bias', 'model.encoder.layers.7.self_attn.linear_v.weight', 'model.encoder.layers.8.conv.batch_norm.bias', 'model.encoder.layers.8.conv.batch_norm.num_batches_tracked', 'model.encoder.layers.8.conv.batch_norm.running_mean', 'model.encoder.layers.8.conv.batch_norm.running_var', 'model.encoder.layers.8.conv.batch_norm.weight', 'model.encoder.layers.8.conv.depthwise_conv.bias', 'model.encoder.layers.8.conv.depthwise_conv.weight', 'model.encoder.layers.8.conv.pointwise_conv1.bias', 'model.encoder.layers.8.conv.pointwise_conv1.weight', 'model.encoder.layers.8.conv.pointwise_conv2.bias', 'model.encoder.layers.8.conv.pointwise_conv2.weight', 'model.encoder.layers.8.feed_forward1.linear1.bias', 'model.encoder.layers.8.feed_forward1.linear1.weight', 'model.encoder.layers.8.feed_forward1.linear2.bias', 'model.encoder.layers.8.feed_forward1.linear2.weight', 'model.encoder.layers.8.feed_forward2.linear1.bias', 'model.encoder.layers.8.feed_forward2.linear1.weight', 'model.encoder.layers.8.feed_forward2.linear2.bias', 'model.encoder.layers.8.feed_forward2.linear2.weight', 'model.encoder.layers.8.norm_conv.bias', 'model.encoder.layers.8.norm_conv.weight', 'model.encoder.layers.8.norm_feed_forward1.bias', 'model.encoder.layers.8.norm_feed_forward1.weight', 'model.encoder.layers.8.norm_feed_forward2.bias', 'model.encoder.layers.8.norm_feed_forward2.weight', 'model.encoder.layers.8.norm_out.bias', 'model.encoder.layers.8.norm_out.weight', 'model.encoder.layers.8.norm_self_att.bias', 'model.encoder.layers.8.norm_self_att.weight', 'model.encoder.layers.8.self_attn.linear_k.bias', 'model.encoder.layers.8.self_attn.linear_k.weight', 'model.encoder.layers.8.self_attn.linear_out.bias', 'model.encoder.layers.8.self_attn.linear_out.weight', 'model.encoder.layers.8.self_attn.linear_q.bias', 'model.encoder.layers.8.self_attn.linear_q.weight', 'model.encoder.layers.8.self_attn.linear_v.bias', 'model.encoder.layers.8.self_attn.linear_v.weight', 'model.encoder.layers.9.conv.batch_norm.bias', 'model.encoder.layers.9.conv.batch_norm.num_batches_tracked', 'model.encoder.layers.9.conv.batch_norm.running_mean', 'model.encoder.layers.9.conv.batch_norm.running_var', 'model.encoder.layers.9.conv.batch_norm.weight', 'model.encoder.layers.9.conv.depthwise_conv.bias', 'model.encoder.layers.9.conv.depthwise_conv.weight', 'model.encoder.layers.9.conv.pointwise_conv1.bias', 'model.encoder.layers.9.conv.pointwise_conv1.weight', 'model.encoder.layers.9.conv.pointwise_conv2.bias', 'model.encoder.layers.9.conv.pointwise_conv2.weight', 'model.encoder.layers.9.feed_forward1.linear1.bias', 'model.encoder.layers.9.feed_forward1.linear1.weight', 'model.encoder.layers.9.feed_forward1.linear2.bias', 'model.encoder.layers.9.feed_forward1.linear2.weight', 'model.encoder.layers.9.feed_forward2.linear1.bias', 'model.encoder.layers.9.feed_forward2.linear1.weight', 'model.encoder.layers.9.feed_forward2.linear2.bias', 'model.encoder.layers.9.feed_forward2.linear2.weight', 'model.encoder.layers.9.norm_conv.bias', 'model.encoder.layers.9.norm_conv.weight', 'model.encoder.layers.9.norm_feed_forward1.bias', 'model.encoder.layers.9.norm_feed_forward1.weight', 'model.encoder.layers.9.norm_feed_forward2.bias', 'model.encoder.layers.9.norm_feed_forward2.weight', 'model.encoder.layers.9.norm_out.bias', 'model.encoder.layers.9.norm_out.weight', 'model.encoder.layers.9.norm_self_att.bias', 'model.encoder.layers.9.norm_self_att.weight', 'model.encoder.layers.9.self_attn.linear_k.bias', 'model.encoder.layers.9.self_attn.linear_k.weight', 'model.encoder.layers.9.self_attn.linear_out.bias', 'model.encoder.layers.9.self_attn.linear_out.weight', 'model.encoder.layers.9.self_attn.linear_q.bias', 'model.encoder.layers.9.self_attn.linear_q.weight', 'model.encoder.layers.9.self_attn.linear_v.bias', 'model.encoder.layers.9.self_attn.linear_v.weight', 'model.encoder.pre_encode.conv.0.bias', 'model.encoder.pre_encode.conv.0.weight', 'model.encoder.pre_encode.conv.2.bias', 'model.encoder.pre_encode.conv.2.weight', 'model.encoder.pre_encode.out.bias', 'model.encoder.pre_encode.out.weight', 'model.head.decoder.embed.weight', 'model.head.decoder.lstm.bias_hh_l0', 'model.head.decoder.lstm.bias_ih_l0', 'model.head.decoder.lstm.weight_hh_l0', 'model.head.decoder.lstm.weight_ih_l0', 'model.head.joint.enc.bias', 'model.head.joint.enc.weight', 'model.head.joint.joint_net.1.bias', 'model.head.joint.joint_net.1.weight', 'model.head.joint.pred.bias', 'model.head.joint.pred.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"nixiieee/gigaam-rnnt-emotion-classifier-dusha\"\n",
    "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = ModelForEmotionClassification.from_pretrained(model_name, config=config, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "cee1102b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3b041d0a6f4abd9b1f10d34e9ce4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1063 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"speech\"]\n",
    "    processed = processor(\n",
    "        audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]\n",
    "    )   \n",
    "    batch[\"input_features\"] = processed[\"input_features\"][0]\n",
    "    batch[\"input_lengths\"] = processed[\"input_lengths\"][0]\n",
    "    batch[\"labels\"] = label_mapping[batch[\"emotion\"]]\n",
    "    return batch\n",
    "\n",
    "processed_resd = resd_dataset.map(prepare_dataset, remove_columns=['speech', 'name', 'path', 'emotion'], num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9481f095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class DataCollatorForEncoderClassificationGigaAM:\n",
    "    def __call__(self, features):\n",
    "        sequences = [torch.tensor(f[\"input_features\"]).T for f in features]\n",
    "\n",
    "        # паддим по первой размерности до max_length\n",
    "        padded = pad_sequence(sequences, batch_first=True, padding_value=0.0)\n",
    "        # теперь padded.shape == [B, max_length, feat_dim]\n",
    "        batch_inputs = padded.transpose(1, 2)\n",
    "        batch_labels = torch.tensor(\n",
    "            [f[\"labels\"] for f in features], dtype=torch.long\n",
    "        )\n",
    "        batch_lens = torch.tensor(\n",
    "            [f[\"input_lengths\"] for f in features], dtype=torch.long\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_features\": batch_inputs,\n",
    "            \"labels\": batch_labels,\n",
    "            \"input_lengths\": batch_lens,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "25800682",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForEncoderClassificationGigaAM()\n",
    "dataloader = DataLoader(processed_resd, batch_size=16, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9232d5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 67/67 [09:48<00:00,  8.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:           28.03%\n",
      "Balanced accuracy:  26.50%\n",
      "Precision:          36.45%\n",
      "Recall:             21.20%\n",
      "F1 Score:           26.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/llm_agent/video_audio_pipeline/emotion_dusha/gigaam_train/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2776: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "/home/llm_agent/video_audio_pipeline/emotion_dusha/gigaam_train/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_model(model, dataloader, device=\"cuda:1\")\n",
    "print(f\"Accuracy:           {metrics['accuracy']:.2%}\")\n",
    "print(f\"Balanced accuracy:  {metrics['balanced_accuracy']:.2%}\")\n",
    "print(f\"Precision:          {metrics['precision']:.2%}\")\n",
    "print(f\"Recall:             {metrics['recall']:.2%}\")\n",
    "print(f\"F1 Score:           {metrics['f1']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcea5cda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gigaam_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
